{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2Jqf2DCij1I"
   },
   "source": [
    "# Importing Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Z5UDXk0oBkAB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hMQ_OtLFF8Ho",
    "outputId": "09a09892-4d8d-44b5-b9f3-4abb86bea0f6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/sw7vd9zs7t1_0_2ww1bq5wr40000gn/T/ipykernel_16358/2267868378.py:2: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  prem_league_data = pd.read_csv('data/cleaned_merged_seasons.csv')\n"
     ]
    }
   ],
   "source": [
    "# Importing the main data\n",
    "prem_league_data = pd.read_csv('data/cleaned_merged_seasons.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezDKnkfhDn_q"
   },
   "source": [
    "# Merging in 2018-19 and 2019-20 seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Qe_AlZp3DAO0"
   },
   "outputs": [],
   "source": [
    "# Reading in the CSVs\n",
    "merged_2018_19 = pd.read_csv('data/2018-19/gws/merged_gw.csv', encoding='iso-8859-1')\n",
    "merged_2019_20 = pd.read_csv('data/2019-20/gws/merged_gw.csv', encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "JSLGcJl9DTqx"
   },
   "outputs": [],
   "source": [
    "# Making a copy of the 2018-19 DF\n",
    "df_19 = merged_2018_19[['name', 'assists', 'bonus', 'bps', 'clean_sheets',\n",
    "                        'creativity', 'element', 'fixture', 'goals_conceded', 'goals_scored', 'ict_index',\n",
    "                        'influence', 'kickoff_time', 'minutes', 'opponent_team', 'own_goals', 'penalties_missed',\n",
    "                        'penalties_saved', 'red_cards', 'round', 'saves', 'selected', 'team_a_score', 'team_h_score',\n",
    "                        'threat', 'total_points', 'transfers_balance', 'transfers_in', 'transfers_out', 'value', 'was_home',\n",
    "                        'yellow_cards', 'GW']].copy()\n",
    "\n",
    "# Correcting issues / missing data in specific columns\n",
    "df_19['season_x'] = '2018-19'\n",
    "df_19['team_x'] = np.nan\n",
    "df_19['position'] = np.nan\n",
    "df_19['name'] = df_19['name'].apply(lambda x: x[:x.rfind('_')].replace('_', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "7ISX-LcmDhMK"
   },
   "outputs": [],
   "source": [
    "# Making a copy of the 2019-20 DF\n",
    "df_20 = merged_2019_20[['name', 'assists', 'bonus', 'bps', 'clean_sheets',\n",
    "                        'creativity', 'element', 'fixture', 'goals_conceded', 'goals_scored', 'ict_index',\n",
    "                        'influence', 'kickoff_time', 'minutes', 'opponent_team', 'own_goals', 'penalties_missed',\n",
    "                        'penalties_saved', 'red_cards', 'round', 'saves', 'selected', 'team_a_score', 'team_h_score',\n",
    "                        'threat', 'total_points', 'transfers_balance', 'transfers_in', 'transfers_out', 'value', 'was_home',\n",
    "                        'yellow_cards', 'GW']].copy()\n",
    "\n",
    "# Correcting issues / missing data in specific columns\n",
    "df_20['season_x'] = '2019-20'\n",
    "df_20['team_x'] = np.nan\n",
    "df_20['position'] = np.nan\n",
    "df_20['name'] = df_20['name'].apply(lambda x: x[:x.rfind('_')].replace('_', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "mO3Bt1c0Eqj6"
   },
   "outputs": [],
   "source": [
    "# Merging the new DFs with the existing DF\n",
    "overall_df = pd.concat([prem_league_data, df_19, df_20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aA2Q0XDE25jQ"
   },
   "source": [
    "# Fixing special characters in names in the 2019-20 season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "n19hihaKvoXq"
   },
   "outputs": [],
   "source": [
    "# Mapping special characters\n",
    "spec_char_dict = {\n",
    "  'Ã¡':'á',\n",
    "  'Ã\\x81':'Á',\n",
    "  'Ã©':'é',\n",
    "  'Ã\\xad':'í',\n",
    "  'Ã³':'ó',\n",
    "  'Ãº':'ú',\n",
    "  'Ã¤':'ä',\n",
    "  'Ã«':'ë',\n",
    "  'Ã¯':'ï',\n",
    "  'Ã¶':'ö',\n",
    "  'Ã\\x96':'Ö',\n",
    "  'Ã¼':'ü',\n",
    "  'Ã£':'ã',\n",
    "  'Ã\\x9f':'ß',\n",
    "  'Ã§':'ç',\n",
    "  'Ä\\x87':'ć',\n",
    "  'Ã\\x87':'Ç',\n",
    "  'Ã±':'ñ',\n",
    "  'Ã¸':'ø',\n",
    "  'Ã\\x98':'Ø',\n",
    "  'Å¡':'š'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "spOunrNS3a_0"
   },
   "outputs": [],
   "source": [
    "# Pulling a list of all the names with special characters to fix\n",
    "# spec_char_names = df['name'][(df['season_x'] == '2019-20') & (df['team_x'].isnull())].unique() - pulled this list initially\n",
    "spec_char_names = ['Abdoulaye DoucourÃ©', 'Adama TraorÃ©',\n",
    "       'AdriÃ¡n San Miguel del Castillo', 'Alexis SÃ¡nchez',\n",
    "       'AndrÃ© Filipe Tavares Gomes', 'Antonio RÃ¼diger', 'Ayoze PÃ©rez',\n",
    "       'Bernard AnÃ\\xadcio Caldeira Duarte', 'Carlos SÃ¡nchez',\n",
    "       'Cheikhou KouyatÃ©', 'CÃ©dric Soares', 'CÃ©sar Azpilicueta',\n",
    "       'Daniel Ceballos FernÃ¡ndez', 'Davinson SÃ¡nchez', 'Davy PrÃ¶pper',\n",
    "       'Djibril SidibÃ©', 'Emiliano BuendÃ\\xada', 'Emiliano MartÃ\\xadnez',\n",
    "       'Fabian SchÃ¤r', 'FabiÃ¡n Balbuena', 'Federico FernÃ¡ndez',\n",
    "       'Francisco FemenÃ\\xada Far', 'FrÃ©dÃ©ric Guilbert', 'GaÃ«tan Bong',\n",
    "       'Georges-KÃ©vin Nkoudou', 'HÃ©ctor BellerÃ\\xadn', 'HÃ©lder Costa',\n",
    "       'Ilkay GÃ¼ndogan', 'IsmaÃ¯la Sarr', 'Javier HernÃ¡ndez BalcÃ¡zar',\n",
    "       'JesÃºs Vallejo LÃ¡zaro', 'Joelinton CÃ¡ssio ApolinÃ¡rio de Lira',\n",
    "       'Jonas LÃ¶ssl', 'Jose Luis Mato SanmartÃ\\xadn',\n",
    "       'JosÃ© Diogo Dalot Teixeira', 'JosÃ© Heriberto Izquierdo Mena',\n",
    "       'JosÃ© Ignacio Peleteiro Romallo',\n",
    "       'JosÃ© Ã\\x81ngel EsmorÃ\\xads Tasende', 'JosÃ© Holebas',\n",
    "       'JoÃ£o Filipe Iria Santos Moutinho', 'JoÃ£o Pedro Cavaco Cancelo',\n",
    "       'JÃ¼rgen Locadia', 'Leroy SanÃ©', 'MartÃ\\xadn Montoya',\n",
    "       'Mesut Ã\\x96zil', 'Miguel AlmirÃ³n', 'Muhamed BeÅ¡iÄ\\x87',\n",
    "       \"N'Golo KantÃ©\", 'Nathan AkÃ©', 'Nicolas PÃ©pÃ©',\n",
    "       'NicolÃ¡s Otamendi', 'Onel HernÃ¡ndez', 'Pascal GroÃ\\x9f',\n",
    "       'Pedro RodrÃ\\xadguez Ledesma', 'Pierre-Emile HÃ¸jbjerg',\n",
    "       'RaÃºl JimÃ©nez', 'Romain SaÃ¯ss',\n",
    "       'Rui Pedro dos Santos PatrÃ\\xadcio', 'RÃºben Diogo da Silva Neves',\n",
    "       'RÃºben GonÃ§alo Silva Nascimento Vinagre', 'Sadio ManÃ©',\n",
    "       'Sebastian PrÃ¶dl', 'Sergio AgÃ¼ero', 'SÃ©bastien Haller',\n",
    "       'Victor LindelÃ¶f', 'VÃ\\xadctor Camarasa', 'Ã\\x87aglar SÃ¶yÃ¼ncÃ¼',\n",
    "       'Ã\\x98rjan Nyland', 'JoÃ£o Pedro Junqueira de Jesus',\n",
    "       'GonÃ§alo Bento Soares Cardoso', 'Bruno AndrÃ© Cavaco Jordao',\n",
    "       'JosÃ© Reina', 'Pablo MarÃ\\xad', 'Borja GonzÃ¡lez TomÃ¡s',\n",
    "       'JoÃ£o Manuel Neves VirgÃ\\xadnia', 'Adalberto PeÃ±aranda']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "AOuBlC5C7bfC"
   },
   "outputs": [],
   "source": [
    "# Fixing special characters\n",
    "fixed_names = []\n",
    "\n",
    "for name in spec_char_names:\n",
    "  for key, value in spec_char_dict.items():\n",
    "      name = name.replace(key, value)\n",
    "  fixed_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "UF_A6tPk7twD"
   },
   "outputs": [],
   "source": [
    "# Making a dictionary of corrected names\n",
    "corrected_name_dict = {spec_char_names[i]: fixed_names[i] for i in range(len(spec_char_names))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "UflLByH-79pI"
   },
   "outputs": [],
   "source": [
    "# Replacing all special character names in overall_df with the corrected names\n",
    "overall_df['name'] = overall_df['name'].replace(corrected_name_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stlflysXkDzf"
   },
   "source": [
    "# Get missing team data for 2016-17, 2017-18, 2018-19, and 2019-20 seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "WkfUXRSJGZsx"
   },
   "outputs": [],
   "source": [
    "# Get list of teams by season\n",
    "teams = pd.read_csv('data/master_team_list.csv')\n",
    "teams_2016 = teams[teams.season=='2016-17']\n",
    "teams_2017 = teams[teams.season=='2017-18']\n",
    "teams_2018 = teams[teams.season=='2018-19']\n",
    "teams_2019 = teams[teams.season=='2019-20']\n",
    "\n",
    "# Get 2016 players and team data\n",
    "players_2016 = pd.read_csv('data/2016-17/players_raw.csv')\n",
    "players_2016['name'] = players_2016.first_name + ' ' + players_2016.second_name\n",
    "players_2016 = players_2016[['name', 'team']]\n",
    "players_2016_merged = players_2016.merge(teams_2016, how='left', on='team')\n",
    "\n",
    "df = overall_df.merge(players_2016_merged, how='left', left_on=['season_x', 'name'], right_on=['season', 'name'])\n",
    "df['team_x'] = np.where(~df['team_name'].isnull(),df['team_name'],df['team_x'])\n",
    "df.drop(columns=['team', 'season', 'team_name'], inplace=True)\n",
    "\n",
    "# Get 2017 players and team data\n",
    "players_2017 = pd.read_csv('data/2017-18/players_raw.csv')\n",
    "players_2017['name'] = players_2017.first_name + ' ' + players_2017.second_name\n",
    "players_2017 = players_2017[['name', 'team']]\n",
    "players_2017_merged = players_2017.merge(teams_2017, how='left', on='team')\n",
    "\n",
    "df = df.merge(players_2017_merged, how='left', left_on=['season_x', 'name'], right_on=['season', 'name'])\n",
    "df['team_x'] = np.where(~df['team_name'].isnull(),df['team_name'], df['team_x'])\n",
    "df.drop(columns=['team', 'season', 'team_name'], inplace=True)\n",
    "\n",
    "# Get 2018 players and team data\n",
    "players_2018 = pd.read_csv('data/2018-19/players_raw.csv')\n",
    "players_2018['name'] = players_2018.first_name + ' ' + players_2018.second_name\n",
    "players_2018 = players_2018[['name', 'team']]\n",
    "players_2018_merged = players_2018.merge(teams_2018, how='left', on='team')\n",
    "\n",
    "df = df.merge(players_2018_merged, how='left', left_on=['season_x', 'name'], right_on=['season', 'name'])\n",
    "df['team_x'] = np.where(~df['team_name'].isnull(),df['team_name'], df['team_x'])\n",
    "df.drop(columns=['team', 'season', 'team_name'], inplace=True)\n",
    "\n",
    "# Get 2019 players and team data\n",
    "players_2019 = pd.read_csv('data/2019-20/players_raw.csv')\n",
    "players_2019['name'] = players_2019.first_name + ' ' + players_2019.second_name\n",
    "players_2019 = players_2019[['name', 'team']]\n",
    "players_2019_merged = players_2019.merge(teams_2019, how='left', on='team')\n",
    "\n",
    "df = df.merge(players_2019_merged, how='left', left_on=['season_x', 'name'], right_on=['season', 'name'])\n",
    "df['team_x'] = np.where(~df['team_name'].isnull(),df['team_name'], df['team_x'])\n",
    "df.drop(columns=['team', 'season', 'team_name'], inplace=True)\n",
    "\n",
    "# Fixing missing team info for David de Gea and Caglar Söyüncü manually\n",
    "mask = ((df.season_x == '2017-18') | (df.season_x == '2018-19')) & ((df.name==\"David de Gea\") | (df.name==\"David De Gea\"))\n",
    "df.loc[mask, 'team_x'] = 'Man Utd'\n",
    "\n",
    "mask = (df.season_x == '2018-19') & (df.name==\"Caglar Söyüncü\")\n",
    "df.loc[mask, 'team_x'] = 'Leicester'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7TF6pynOivO"
   },
   "source": [
    "# Fill Missing Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "JyHMK6hEy-OF"
   },
   "outputs": [],
   "source": [
    "fill_pos = pd.read_csv('data/mpos.csv')\n",
    "fill_pos.columns = ['dirty_name', 'clean_name', 'scrape_position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "wH0G8X57sBrw"
   },
   "outputs": [],
   "source": [
    "# clean names again\n",
    "df['name'] = df['name'].replace(corrected_name_dict)\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "def normalize_string(text):\n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = ' '.join(text.split())\n",
    "    # Normalize unicode characters to the closest ASCII representation\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n",
    "    return text\n",
    "\n",
    "# Normalizing names, easier than removing special chars, suggest we use the normalized name col\n",
    "df['name_normalized'] = df['name'].apply(normalize_string)\n",
    "fill_pos['name_normalized'] = fill_pos['clean_name'].apply(normalize_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "hbG1C_HPI6W-"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to use for filling in missing names\n",
    "fill_pos[['name_normalized', 'scrape_position']].to_dict()\n",
    "pos_dict = pd.Series(fill_pos['scrape_position'].values, index=fill_pos['name_normalized']).to_dict()\n",
    "# Katherine note - why are these here?\n",
    "pos_dict.update({\n",
    "    'greg cunninghamm': 'DEF',\n",
    "    'muhamed besic': 'MID',\n",
    "    'zeze steven sessegnon': 'DEF',\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "irIhfMuDwYb7"
   },
   "outputs": [],
   "source": [
    "# Filling nulls with a \"missing position\" tag\n",
    "df['position'] = df['position'].fillna('no_pos')\n",
    "\n",
    "# Filling missing positions using the dictionary created above\n",
    "def fill_missing_position(row):\n",
    "  if row['position'] == 'no_pos':\n",
    "    row['position'] = pos_dict.get(row['name_normalized'], \"no_position\")\n",
    "  return row\n",
    "\n",
    "df = df.apply(fill_missing_position, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning GK data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up GK positions\n",
    "df.loc[ df['position'] == 'GKP', 'position'] = 'GK'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7weZdn3nEAnQ"
   },
   "source": [
    "# Renaming messy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "a543AdOsEABc"
   },
   "outputs": [],
   "source": [
    "df.rename(columns={\"season_x\": \"season\", \"name\": \"player_name\", \"team_x\": \"player_team_name\", \n",
    "                   \"opponent_team\": \"opp_team_id\", 'position_x':'position'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mm5Rzwx-nyHI"
   },
   "source": [
    "# Adding missing opponent team names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "N2AnbiuM358_"
   },
   "outputs": [],
   "source": [
    "# Merging in team info - joins opponent team name (team_name) based on opp_team_id, which is populated in the seasons missing opponent team name\n",
    "df = df.merge(teams, how='left', left_on=['season','opp_team_id'], right_on=['season','team'])\n",
    "\n",
    "# Dropping unnecessary columns\n",
    "df.drop(columns=['team','opp_team_name'], inplace=True)\n",
    "\n",
    "# Renaming opponent team name column\n",
    "df.rename(columns={\"team_name\": \"opp_team_name\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8V44HsrqBkqZ"
   },
   "source": [
    "# Adding opponent difficulty column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "eUUTosdaBhKB"
   },
   "outputs": [],
   "source": [
    "# Importing difficulty data\n",
    "difficulty_data = pd.read_csv('data/team_difficulty_ind.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "8TqwrCHr7V9y"
   },
   "outputs": [],
   "source": [
    "# Joining difficulty data to ours\n",
    "df = df.merge(difficulty_data, how='left', left_on='opp_team_name', right_on='team_name').drop(columns = ['team_name'])\n",
    "\n",
    "# Renaming the opponent difficulty column\n",
    "df.rename(columns={\"FDI\": \"opp_diff_ind\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnjsCtrxFvd2"
   },
   "source": [
    "# Transforming the \"away and home team score\" columns into a more usable form (player team and opponent team score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "1CgsSoNLFuqk"
   },
   "outputs": [],
   "source": [
    "# Need two new columns: player_team_score and opp_team_score. Populated from team_a_score and team_h_score based on was_home flag\n",
    "df['player_team_score'] = np.where(df['was_home'] == True, df['team_h_score'], df['team_a_score'])\n",
    "df['opp_team_score'] = np.where(df['was_home'] == True, df['team_a_score'], df['team_h_score'])\n",
    "\n",
    "# Drop team_a_score and team_h_score once we have these two columns\n",
    "df.drop(columns=['team_a_score','team_h_score'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data from COVID affected season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust GW 2019-20 for COVID affected Gameweeks (GW 30+) to convert to 38 GW season\n",
    "\n",
    "# Update the GW values for the 2019-20 season\n",
    "df.loc[(df['season'] == '2019-20') & (df['GW'].isin(range(39, 48))), 'GW'] = df['GW'] - 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIg0-oewBjq7"
   },
   "source": [
    "# Making lagged average columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "Pga6mB6YBg0p"
   },
   "outputs": [],
   "source": [
    "# Function to make a DF with lagged features; can input features to lag and number of weeks to lag them\n",
    "def create_lagged_features(df, lag_columns, lag_weeks=3):\n",
    "    # Function to calculate lagged features for a given player and season\n",
    "    def calculate_lags(player_df):\n",
    "        player_df = player_df.sort_values('GW')\n",
    "        for col in lag_columns:\n",
    "            player_df[f'{col}_lag_{lag_weeks}'] = player_df[col].rolling(window=lag_weeks, min_periods=1).mean().shift()\n",
    "        return player_df\n",
    "\n",
    "    lagged_df = df.groupby(['season', 'player_name']).apply(calculate_lags).reset_index(drop=True)\n",
    "\n",
    "    return lagged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "F1VxUInZDe6U"
   },
   "outputs": [],
   "source": [
    "# Columns that we want to lag\n",
    "lag_columns = ['assists', 'bonus', 'bps', 'clean_sheets', 'creativity',\n",
    "               'goals_conceded','goals_scored', 'ict_index', 'influence',\n",
    "               'minutes', 'own_goals', 'penalties_missed', 'penalties_saved',\n",
    "               'red_cards', 'saves', 'selected','player_team_score', 'opp_team_score',\n",
    "               'threat', 'total_points', 'value', 'yellow_cards']\n",
    "\n",
    "# Excluding was_home and opp_diff_ind as these are categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "d0sHQpgUq6DM"
   },
   "outputs": [],
   "source": [
    "# Creating one week lag - will take ~1 minute\n",
    "lagged_df = create_lagged_features(df, lag_columns, lag_weeks = 1)\n",
    "lagged_df = lagged_df.fillna(0)\n",
    "\n",
    "# Replacing current DF with the lagged DF since it includes all of our columns and more\n",
    "df = lagged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "N8fVthqlqNPd"
   },
   "outputs": [],
   "source": [
    "# Creating three week lag - will take ~1 minute\n",
    "lagged_df_three_wk = create_lagged_features(df, lag_columns, lag_weeks = 3)\n",
    "lagged_df_three_wk = lagged_df_three_wk.fillna(0)\n",
    "\n",
    "# Replacing current DF with the lagged DF since it includes all of our columns and more\n",
    "df = lagged_df_three_wk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "5Fe4uOCsxGiw"
   },
   "outputs": [],
   "source": [
    "# Creating five week lag - will take ~1 minute\n",
    "lagged_df_five_wk = create_lagged_features(df, lag_columns, lag_weeks = 5)\n",
    "lagged_df_five_wk = lagged_df_five_wk.fillna(0)\n",
    "\n",
    "# Replacing current DF with the lagged DF since it includes all of our columns and more\n",
    "df = lagged_df_five_wk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQu27rZqfGRC"
   },
   "source": [
    "# Sequence data by season + game week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "sn2QTJTMfGfz"
   },
   "outputs": [],
   "source": [
    "df = df.sort_values(['season','player_name','GW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhqroJAI5CUd"
   },
   "source": [
    "# Dropping columns we won't use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "ARnVeAwpLjOV"
   },
   "outputs": [],
   "source": [
    "# Dropping columns that aren't useful\n",
    "df.drop(columns=['player_name', 'kickoff_time', 'opp_team_id', 'round', 'element', 'fixture', \n",
    "                 'transfers_balance','transfers_in' ,'transfers_out'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "nhmnxEtn5CbL"
   },
   "outputs": [],
   "source": [
    "# Dropping all unlagged continuous columns except the target\n",
    "lag_columns.remove('total_points')\n",
    "df.drop(columns=lag_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create two test teams for analysis of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "Nl5RJRkvX8fe"
   },
   "outputs": [],
   "source": [
    "df_man_city = df[(df['season'] == '2022-23') & (df['player_team_name'] == 'Man City')]\n",
    "df_brighton = df[(df['season'] == '2022-23') & (df['player_team_name'] == 'Brighton')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHh4Jyf1UPxp"
   },
   "source": [
    "# Split data into train/validation/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "hf-fhf_rUP4w"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/sw7vd9zs7t1_0_2ww1bq5wr40000gn/T/ipykernel_16358/3771032746.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_init.drop(columns=['total_points'], inplace=True)\n",
      "/var/folders/tc/sw7vd9zs7t1_0_2ww1bq5wr40000gn/T/ipykernel_16358/3771032746.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val_init.drop(columns=['total_points'], inplace=True)\n",
      "/var/folders/tc/sw7vd9zs7t1_0_2ww1bq5wr40000gn/T/ipykernel_16358/3771032746.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_init.drop(columns=['total_points'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "X_train_init = df[df['season'].isin(['2018-19','2019-20','2020-21'])]\n",
    "Y_train = X_train_init[['name_normalized', 'total_points']]\n",
    "X_train_init.drop(columns=['total_points'], inplace=True)\n",
    "# Y_train = np.array(X_train_init.pop('total_points'))\n",
    "\n",
    "# Validation\n",
    "X_val_init = df[df['season'].isin(['2021-22'])]\n",
    "Y_val = X_val_init[['name_normalized', 'total_points']]\n",
    "X_val_init.drop(columns=['total_points'], inplace=True)\n",
    "# Y_val = np.array(X_val_init.pop('total_points'))\n",
    "\n",
    "\n",
    "# Test\n",
    "X_test_init = df[df['season'].isin(['2022-23'])]\n",
    "Y_test = X_test_init[['name_normalized', 'total_points']]\n",
    "X_test_init.drop(columns=['total_points'], inplace=True)\n",
    "\n",
    "# Ignore this warning - it's working as intended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8vh8gqG_NLO"
   },
   "source": [
    "# Standardize lagged features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "IQ9VP4HA6pVJ"
   },
   "outputs": [],
   "source": [
    "# Main train/val/test split\n",
    "# Pulling out only the features to be standardized\n",
    "categorical_vars = ['season','player_team_name','opp_team_name','opp_diff_ind','position','GW','was_home', 'name_normalized']\n",
    "\n",
    "X_train_contin = X_train_init.loc[:, ~X_train_init.columns.isin(categorical_vars)]\n",
    "X_val_contin = X_val_init.loc[:, ~X_val_init.columns.isin(categorical_vars)]\n",
    "X_test_contin = X_test_init.loc[:, ~X_test_init.columns.isin(categorical_vars)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two team-level test datasets - delete at the end\n",
    "categorical_vars_test = ['name_normalized','season','player_team_name','opp_team_name','opp_diff_ind','position','GW','was_home',\n",
    "                         'total_points']\n",
    "df_man_city_contin = df_man_city.loc[:, ~df_man_city.columns.isin(categorical_vars_test)]\n",
    "df_brighton_contin = df_brighton.loc[:, ~df_brighton.columns.isin(categorical_vars_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "JhyLTO-pT_q4"
   },
   "outputs": [],
   "source": [
    "# Standardizing the continuous variables\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the train features\n",
    "train_features = scaler.fit_transform(X_train_contin)\n",
    "\n",
    "# Only transform the validation and test features\n",
    "val_features = scaler.transform(X_val_contin)\n",
    "test_features = scaler.transform(X_test_contin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize test team features - delete at the end\n",
    "df_man_city_feat = scaler.transform(df_man_city_contin)\n",
    "df_brighton_feat = scaler.transform(df_brighton_contin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "ouwxQ-LR-Hm8"
   },
   "outputs": [],
   "source": [
    "# Turn the standardized arrays back into DFs and add the column names back in\n",
    "X_train_temp = pd.DataFrame(train_features, columns=X_train_contin.columns)\n",
    "X_val_temp = pd.DataFrame(val_features, columns=X_val_contin.columns)\n",
    "X_test_temp = pd.DataFrame(test_features, columns=X_test_contin.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the standardized arrays back into DFs and add the column names back in (for two test teams) - delete at the end\n",
    "df_man_city_temp = pd.DataFrame(df_man_city_feat, columns=df_man_city_contin.columns)\n",
    "df_brighton_temp = pd.DataFrame(df_brighton_feat, columns=df_brighton_contin.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "73brhBfuAXdB"
   },
   "outputs": [],
   "source": [
    "# Join the categorical variables back\n",
    "X_train_ = pd.concat([X_train_init[categorical_vars].reset_index(drop=True),X_train_temp], axis=1)\n",
    "X_val_ = pd.concat([X_val_init[categorical_vars].reset_index(drop=True),X_val_temp], axis=1)\n",
    "X_test_ = pd.concat([X_test_init[categorical_vars].reset_index(drop=True),X_test_temp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the categorical variables back (for two test teams) - delete at the end\n",
    "df_man_city = pd.concat([df_man_city[categorical_vars_test].reset_index(drop=True),df_man_city_temp], axis=1)\n",
    "df_brighton = pd.concat([df_brighton[categorical_vars_test].reset_index(drop=True),df_brighton_temp], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKOKuir02Gua"
   },
   "source": [
    "# Exporting CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "Xe_WJHN8fTyB"
   },
   "outputs": [],
   "source": [
    "X_train_.to_csv('data/X_train.csv', encoding='utf-8')\n",
    "Y_train.to_csv('data/Y_train.csv', encoding='utf-8')\n",
    "X_val_.to_csv('data/X_val.csv', encoding='utf-8')\n",
    "Y_val.to_csv('data/Y_val.csv', encoding='utf-8')\n",
    "X_test_.to_csv('data/X_test.csv', encoding='utf-8')\n",
    "Y_test.to_csv('data/Y_test.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "YUU4gC0qMgtu"
   },
   "outputs": [],
   "source": [
    "# Two test teams - delete at the end\n",
    "df_man_city.to_csv('data/df_man_city_test.csv', encoding='utf-8')\n",
    "df_brighton.to_csv('data/df_brighton_test.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
